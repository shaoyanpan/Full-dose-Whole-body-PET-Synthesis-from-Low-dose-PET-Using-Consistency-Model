{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30c3773b",
   "metadata": {},
   "source": [
    "# Here are the library you need to import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8361ccfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import glob\n",
    "import scipy.io\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "import numpy as np\n",
    "from natsort import natsorted\n",
    "from monai.transforms import (\n",
    "    AsDiscrete,\n",
    "    AddChanneld,\n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    RandFlipd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    RandShiftIntensityd,\n",
    "    ScaleIntensityRanged,\n",
    "    Spacingd,\n",
    "    RandRotate90d,\n",
    "    ToTensord,\n",
    "    RandAffined,\n",
    "    RandCropByLabelClassesd,\n",
    "    SpatialPadd,\n",
    "    RandAdjustContrastd,\n",
    "    RandShiftIntensityd,\n",
    "    ScaleIntensityd,\n",
    "    NormalizeIntensityd,\n",
    "    RandScaleIntensityd,\n",
    "    RandGaussianNoised,\n",
    "    RandGaussianSmoothd,\n",
    "    ScaleIntensityRangePercentilesd,\n",
    "    Resized,\n",
    "    Transposed,\n",
    "    RandSpatialCropd,\n",
    "    RandSpatialCropSamplesd,\n",
    "    ResizeWithPadOrCropd\n",
    ")\n",
    "from monai.transforms import (CastToTyped,\n",
    "                              Compose, CropForegroundd, EnsureChannelFirstd, LoadImaged,\n",
    "                              NormalizeIntensity, RandCropByPosNegLabeld,\n",
    "                              RandFlipd, RandGaussianNoised,\n",
    "                              RandGaussianSmoothd, RandScaleIntensityd,\n",
    "                              RandZoomd, SpatialCrop, SpatialPadd, EnsureTyped)\n",
    "import copy\n",
    "from cm.karras_diffusion import KarrasDenoiser,karras_sample\n",
    "from cm.script_util import (\n",
    "    create_ema_and_scales_fn,\n",
    ")\n",
    "\n",
    "from cm.fp16_util import (\n",
    "    MixedPrecisionTrainer,\n",
    "    get_param_groups_and_shapes,\n",
    "    make_master_params,\n",
    "    master_params_to_model_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39e4aa7",
   "metadata": {},
   "source": [
    "# Build the data loader using the monai library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a750f5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are the dataloader hyper-parameters, including the batch size,\n",
    "# image full size, patch size, image spacing, and color channel (usually 1 for medical images)\n",
    "BATCH_SIZE_TRAIN = 20*1\n",
    "BATCH_SIZE_TEST = 1\n",
    "img_full_size = (96,192)\n",
    "patch_width = 64\n",
    "patch_size = (patch_width,patch_width)\n",
    "spacing = (1,1)\n",
    "patch_num = 2\n",
    "channels = 1\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0eccbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are the dataloader class if your data are .mat files which contains both image and label in a single file. For nii.gz files, see the next block.\n",
    "# Please see the comment below for your data pre-processing.\n",
    "# load image-> add channel dimension to the image -> intensity normalization \n",
    "# -> padding or crop the boundary to ensure all images have same size -> extract random patches\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,imgs_path,labels_path=None, train_flag = True):\n",
    "        self.imgs_path = imgs_path\n",
    "        self.labels_path = labels_path\n",
    "        self.train_flag = train_flag\n",
    "        files_list = []\n",
    "        labels_list = []\n",
    "        for img_path in imgs_path:\n",
    "            file_list = natsorted(glob.glob(img_path + \"*mat\"), key=lambda y: y.lower())     \n",
    "            files_list += file_list\n",
    "        self.data = []\n",
    "        self.label = []\n",
    "        for img_path in files_list:\n",
    "            class_name = img_path.split(\"/\")[-1]\n",
    "            self.data.append([img_path, class_name])\n",
    "\n",
    "        # Now we start to see what data preprocessing we need  \n",
    "        self.train_transforms = Compose(\n",
    "                [\n",
    "                    # Add a new dimension to channel. Must have.\n",
    "                    AddChanneld(keys=[\"image\",\"label\"]),\n",
    "                    \n",
    "                    # Data normalization: -1 to 1\n",
    "#                     ScaleIntensityd(keys=[\"image\",\"label\"], minv=-1, maxv=1.0),\n",
    "                    \n",
    "                    # Pad or crop all images to a uniform size\n",
    "                    ResizeWithPadOrCropd(\n",
    "                          keys=[\"image\",\"label\"],\n",
    "                          spatial_size = img_full_size,\n",
    "                          constant_values = -1,\n",
    "                    ),\n",
    "                    \n",
    "                    # Randomly extract several patches from the input image\n",
    "                    RandSpatialCropSamplesd(keys=[\"image\",\"label\"],\n",
    "                          roi_size = patch_size,\n",
    "                          num_samples = patch_num,\n",
    "                          random_size=False,\n",
    "                          ),\n",
    "                    ToTensord(keys=[\"image\",\"label\"]),\n",
    "                ]\n",
    "            )\n",
    "        self.test_transforms = Compose(\n",
    "                [\n",
    "                    AddChanneld(keys=[\"image\",\"label\"]),\n",
    "#                     ScaleIntensityd(keys=[\"image\",\"label\"], minv=-1, maxv=1.0),\n",
    "                    ResizeWithPadOrCropd(\n",
    "                          keys=[\"image\",\"label\"],\n",
    "                          spatial_size = img_full_size,\n",
    "                          constant_values = -1,\n",
    "                    ),\n",
    "                    ToTensord(keys=[\"image\",\"label\"]),\n",
    "                ]\n",
    "            ) \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, class_name = self.data[idx]\n",
    "        cao = scipy.io.loadmat(img_path)\n",
    "        if not self.train_flag:\n",
    "            affined_data_dict = self.test_transforms(cao)   \n",
    "            img_tensor = affined_data_dict['image'].to(torch.float)\n",
    "            label_tensor = affined_data_dict['label'].to(torch.float)   \n",
    "        else:\n",
    "            affined_data_dict = self.train_transforms(cao)   \n",
    "            img = np.zeros([patch_num, patch_width,patch_width])\n",
    "            label = np.zeros([patch_num, patch_width,patch_width])\n",
    "            for i,after_l in enumerate(affined_data_dict):\n",
    "                img[i,:,:] = after_l['image'].squeeze()\n",
    "                label[i,:,:] = after_l['label'].squeeze()\n",
    "            img_tensor = torch.from_numpy(img).to(torch.float)\n",
    "            label_tensor = torch.from_numpy(label).to(torch.float)\n",
    "        \n",
    "        data_tensor = {'image':img_tensor,'label':label_tensor}\n",
    "        return data_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1b3a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Here we use monai to process the nii data. Compare to the mat data, the nii data can have orientation and spacing information.\n",
    "# # However, compare to .mat data, we must give image and label paths seperately.\n",
    "# # load image using nibabelreader (read nii) -> add channel dimension to the image -> ensure orientation -> respacing all image to\n",
    "# # a same spacing -> intensity normalization -> padding or crop the boundary to ensure all images have same size\n",
    "\n",
    "# class CustomDataset(Dataset):\n",
    "#     def __init__(self,imgs_path,labels_path, train_flag = True):\n",
    "#         self.imgs_path = imgs_path\n",
    "#         self.labels_path = labels_path\n",
    "#         self.train_flag = train_flag\n",
    "#         files_list = []\n",
    "#         labels_list = []\n",
    "#         for img_path,label_path in zip(imgs_path,labels_path):\n",
    "#             file_list = natsorted(glob.glob(img_path + \"*mat\"), key=lambda y: y.lower())     \n",
    "#             label_list = natsorted(glob.glob(label_path + \"*mat\"), key=lambda y: y.lower())\n",
    "#             files_list += file_list\n",
    "#             labels_list += label_list\n",
    "#         self.data = []\n",
    "#         self.label = []\n",
    "#         for img_path in files_list:\n",
    "#             class_name = img_path.split(\"/\")[-1]\n",
    "#             self.data.append([img_path, class_name])\n",
    "#         for label_path in labels_list:\n",
    "#             class_name = label_path.split(\"/\")[-1]\n",
    "#             self.label.append([label_path, class_name])\n",
    "            \n",
    "            \n",
    "#         # Now we start to see what data preprocessing we need  \n",
    "#         self.train_transforms = Compose(\n",
    "#                 [\n",
    "#                     # Read data\n",
    "#                     LoadImaged(keys=[\"image\",\"label\"],reader='nibabelreader'),\n",
    "                    \n",
    "#                     # Add a new dimension to channel. Must have.\n",
    "#                     AddChanneld(keys=[\"image\",\"label\"]),\n",
    "                    \n",
    "#                     # Automatically align the orientation of all patients.\n",
    "#                     Orientationd(keys=[\"image\"], axcodes=\"RAS\"),\n",
    "                    \n",
    "#                     # Automatically align the pixel spacing of all patients.\n",
    "#                     Spacingd(\n",
    "#                         keys=[\"image\"],\n",
    "#                         pixdim=spacing,\n",
    "#                         mode=(\"bilinear\"),\n",
    "#                     ),\n",
    "                    \n",
    "#                     # Data normalization: -1 to 1\n",
    "#                     ScaleIntensityd(keys=[\"image\",\"label\"], minv=-1, maxv=1.0),\n",
    "                    \n",
    "#                     # Pad or crop all images to a uniform size\n",
    "#                     ResizeWithPadOrCropd(\n",
    "#                           keys=[\"image\",\"label\"],\n",
    "#                           spatial_size = img_full_size,\n",
    "#                           constant_values = -1,\n",
    "#                     ),\n",
    "                    \n",
    "#                     # Randomly extract several patches from the input image\n",
    "#                     RandSpatialCropSamplesd(keys=[\"image\",\"label\"],\n",
    "#                           roi_size = (image_size,image_size),\n",
    "#                           num_samples = patch_num,\n",
    "#                           random_size=False,\n",
    "#                           ),\n",
    "#                     ToTensord(keys=[\"image\",\"label\"]),\n",
    "#                 ]\n",
    "#             )\n",
    "#         self.test_transforms = Compose(\n",
    "#                 [\n",
    "#                     LoadImaged(keys=[\"image\",\"label\"],reader='nibabelreader'),\n",
    "#                     AddChanneld(keys=[\"image\",\"label\"]),\n",
    "#                     Orientationd(keys=[\"image\"], axcodes=\"RAS\"),\n",
    "#                     Spacingd(\n",
    "#                         keys=[\"image\"],\n",
    "#                         pixdim=spacing,\n",
    "#                         mode=(\"bilinear\"),\n",
    "#                     ),\n",
    "#                     ScaleIntensityd(keys=[\"image\",\"label\"], minv=-1, maxv=1.0),\n",
    "#                     ResizeWithPadOrCropd(\n",
    "#                           keys=[\"image\",\"label\"],\n",
    "#                           spatial_size = img_full_size,\n",
    "#                           constant_values = -1,\n",
    "#                     ),\n",
    "#                     ToTensord(keys=[\"image\",\"label\"]),\n",
    "#                 ]\n",
    "#             ) \n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         img_path, class_name = self.data[idx]\n",
    "#         label_path, class_name = self.label[idx]\n",
    "#         data_dict = {\"image\":img_path,'label':label_path}\n",
    "#         if not self.train_flag:\n",
    "#             affined_data_dict = self.test_transforms(data_dict)   \n",
    "#             img_tensor = affined_data_dict['image'].to(torch.float)\n",
    "#             label_tensor = affined_data_dict['label'].to(torch.float)   \n",
    "#         else:\n",
    "#             affined_data_dict = self.train_transforms(data_dict)   \n",
    "#             img = np.zeros([patch_num, image_size,image_size])\n",
    "#             label = np.zeros([patch_num, image_size,image_size])\n",
    "#             for i,after_l in enumerate(affined_data_dict):\n",
    "#                 img[i,:,:] = after_l['image'].squeeze()\n",
    "#                 label[i,:,:] = after_l['label'].squeeze()\n",
    "#             img_tensor = torch.from_numpy(img).to(torch.float)\n",
    "#             label_tensor = torch.from_numpy(label).to(torch.float)\n",
    "        \n",
    "#         data_tensor = {'image':img_tensor,'label':label_tensor}\n",
    "#         return data_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2e1924",
   "metadata": {},
   "source": [
    "# Build the Consistency-Model's network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6250173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here enter your network parameters:\n",
    "# num_channels means the initial channels in each block,128 here.\n",
    "# Length of the channel_mult means the layer#, 4 here.\n",
    "# channel_mult means the multipliers of the channels (in this case, 128,256,384,512 for the first to the fourth block),\n",
    "# attention_resulution means we use the transformer blocks in the third to the fourth block\n",
    "# number of heads, window size in each transformer block\n",
    "# \n",
    "num_channels=128\n",
    "attention_resolutions=\"16,8\"\n",
    "channel_mult = (1, 2, 3, 4)\n",
    "num_heads=[4,4,8,16]\n",
    "window_size = [[4,4],[4,4],[4,4],[4,4]]\n",
    "num_res_blocks = [2,2,2,2]\n",
    "sample_kernel=([2,2],[2,2],[2,2]),\n",
    "\n",
    "attention_ds = []\n",
    "for res in attention_resolutions.split(\",\"):\n",
    "    attention_ds.append(patch_width//int(res))\n",
    "class_cond = False\n",
    "use_scale_shift_norm = True\n",
    "\n",
    "# from Network.Diffusion_model_transformer import *\n",
    "# Consistency_network = SwinVITModel(\n",
    "#         image_size=patch_size,\n",
    "#         in_channels=2,\n",
    "#         model_channels=num_channels,\n",
    "#         out_channels=1,\n",
    "#         dims=2,\n",
    "#         sample_kernel = sample_kernel,\n",
    "#         num_res_blocks=num_res_blocks,\n",
    "#         attention_resolutions=tuple(attention_ds),\n",
    "#         dropout=0,\n",
    "#         channel_mult=channel_mult,\n",
    "#         num_classes=None,\n",
    "#         use_checkpoint=False,\n",
    "#         use_fp16=False,\n",
    "#         num_heads=num_heads,\n",
    "#         window_size = window_size,\n",
    "#         num_head_channels=64,\n",
    "#         num_heads_upsample=-1,\n",
    "#         use_scale_shift_norm=use_scale_shift_norm,\n",
    "#         resblock_updown=False,\n",
    "#         use_new_attention_order=False,\n",
    "#     ).to(device)\n",
    "\n",
    "# # Don't forget the target model. You must have this to run the code no matter you use ema or not.\n",
    "# Consistency_network_target = copy.deepcopy(Consistency_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0393fe2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The Unet we provide here if you want to use it\n",
    "from Network.Diffusion_model_Unet_2d import *\n",
    "Consistency_network = UNetModel(\n",
    "        img_size = patch_size,\n",
    "        image_size=patch_width,\n",
    "        in_channels=2,\n",
    "        model_channels=num_channels,\n",
    "        out_channels=1,\n",
    "        dims = 2,\n",
    "        num_res_blocks=num_res_blocks[0],\n",
    "        attention_resolutions=tuple(attention_ds),\n",
    "        dropout=0.,\n",
    "        sample_kernel=sample_kernel,\n",
    "        channel_mult=channel_mult,\n",
    "        num_classes=(NUM_CLASSES if class_cond else None),\n",
    "        use_checkpoint=False,\n",
    "        use_fp16=False,\n",
    "        num_heads=4,\n",
    "        num_head_channels=64,\n",
    "        num_heads_upsample=-1,\n",
    "        use_scale_shift_norm=use_scale_shift_norm,\n",
    "        resblock_updown=True,\n",
    "        use_new_attention_order=False,\n",
    "    ).to(device)\n",
    "\n",
    "# Don't forget the target model. You must have this to run the code no matter you use ema or not.\n",
    "Consistency_network_target = copy.deepcopy(Consistency_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236901dd",
   "metadata": {},
   "source": [
    "# Create the consistency-diffusion process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cf9e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the hyper-parameters of the consistency model\n",
    "consistency = KarrasDenoiser(        \n",
    "        sigma_data=0.5,\n",
    "        sigma_max=80.0,\n",
    "        sigma_min=0.002,\n",
    "        rho=7.0,\n",
    "        weight_schedule=\"karras\",\n",
    "        distillation=False,\n",
    "        loss_norm=\"l1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d12c7c5",
   "metadata": {},
   "source": [
    "# Call the ema optimizer and ready for start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e983d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consistency start and end steps in the ema optimization process\n",
    "\n",
    "start_scales = 2\n",
    "end_scales = 150\n",
    "metric = torch.nn.L1Loss()\n",
    "ema_scale_fn = create_ema_and_scales_fn(\n",
    "    target_ema_mode='adaptive',\n",
    "    start_ema=0.95,\n",
    "    scale_mode='progressive',\n",
    "    start_scales=start_scales,\n",
    "    end_scales=end_scales,\n",
    "    total_steps=800000,\n",
    "    distill_steps_per_iter=5000,\n",
    ")\n",
    "\n",
    "ema_rate  = \"0.9999,0.99994,0.9999432189950708\"\n",
    "real_ema_rate = (\n",
    "            [ema_rate]\n",
    "            if isinstance(ema_rate, float)\n",
    "            else [float(x) for x in ema_rate.split(\",\")]\n",
    "        )\n",
    "mp_trainer = MixedPrecisionTrainer(\n",
    "            model=Consistency_network,\n",
    "            use_fp16=True,\n",
    "            fp16_scale_growth=False,\n",
    "        )\n",
    "Consistency_model_optimizer = torch.optim.RAdam(mp_trainer.master_params, lr=1e-5,weight_decay = 1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb44eb1",
   "metadata": {},
   "source": [
    "# Build the training function. Run the training function once = one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6283a4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we Consistency_network the training process\n",
    "def train(Consistency_network,Consistency_network_target, Consistency_model_optimizer,data_loader1, consistency,global_step):\n",
    "    \n",
    "    #1: set the network and its ema network to training mode\n",
    "    Consistency_network.train()\n",
    "    Consistency_network_target.requires_grad_(False)\n",
    "    Consistency_network_target.train()\n",
    "    total_samples = len(data_loader1.dataset)\n",
    "    A_to_B_loss_sum = []\n",
    "    total_time = 0\n",
    "    aa = time.time()\n",
    "    for i, data in enumerate(data_loader1):\n",
    "        \n",
    "        #2: Loop the whole dataset, condition (low dose PET, input) and target (high dose PET, target)\n",
    "        condition = data['image'].view(-1,1,patch_width,patch_width).to(device)\n",
    "        target = data['label'].view(-1,1,patch_width,patch_width).to(device)\n",
    "        \n",
    "        #3: EMA optimizations setup\n",
    "        A_to_B_param_groups_and_shapes = get_param_groups_and_shapes(\n",
    "            Consistency_network.named_parameters()\n",
    "        )\n",
    "        A_to_B_master_params = make_master_params(A_to_B_param_groups_and_shapes)\n",
    "\n",
    "        ema_params = [\n",
    "            copy.deepcopy(A_to_B_master_params)\n",
    "            for _ in range(len(real_ema_rate))\n",
    "        ]\n",
    "        \n",
    "        ema, num_scales = ema_scale_fn(global_step)\n",
    "        \n",
    "                \n",
    "        #4: Optimize the Consistency_network to correct perform the consistency process\n",
    "        mp_trainer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            all_loss = consistency.consistency_losses(Consistency_network,\n",
    "                        target,\n",
    "                        condition,\n",
    "                        num_scales,\n",
    "                        target_model=Consistency_network_target)\n",
    "            \n",
    "            A_to_B_loss = all_loss['loss'].mean()\n",
    "            A_to_B_loss_sum.append(A_to_B_loss.detach().cpu().numpy())\n",
    "            \n",
    "        mp_trainer.backward(A_to_B_loss)\n",
    "        mp_trainer.optimize(Consistency_model_optimizer)\n",
    "\n",
    "        #5: _update_ema()\n",
    "        for rate, params in zip(real_ema_rate, ema_params):\n",
    "            for targ, src in zip(params, A_to_B_master_params):\n",
    "                targ.detach().mul_(0.99).add_(src, alpha=1 - 0.99)\n",
    "        \n",
    "        target_model_param_groups_and_shapes = get_param_groups_and_shapes(\n",
    "            Consistency_network_target.named_parameters()\n",
    "        )\n",
    "        target_model_master_params = make_master_params(\n",
    "            target_model_param_groups_and_shapes\n",
    "        )\n",
    "        \n",
    "        target_ema, scales = ema_scale_fn(global_step)\n",
    "        with torch.no_grad():\n",
    "            for targ, src in zip(target_model_master_params, A_to_B_master_params):\n",
    "                targ.detach().mul_(target_ema).add_(src, alpha=1 - target_ema)\n",
    "\n",
    "            master_params_to_model_params(\n",
    "                target_model_param_groups_and_shapes,\n",
    "                target_model_master_params,\n",
    "            )\n",
    "\n",
    "        total_time += time.time()-aa\n",
    "        \n",
    "        \n",
    "        #6: Print out loss for every 100 iteration, just for monitoning \n",
    "        global_step += 1\n",
    "        if i % 100 == 0:\n",
    "            print('optimization time: '+ str(time.time()-aa))\n",
    "            print('[' +  '{:5}'.format(i * BATCH_SIZE_TRAIN) + '/' + '{:5}'.format(total_samples) +\n",
    "                  ' (' + '{:3.0f}'.format(100 * i / len(data_loader1)) + '%)]  A_to_B_Loss: ' +\n",
    "                  '{:6.7f}'.format(np.nanmean(A_to_B_loss_sum)))\n",
    "\n",
    "        \n",
    "    average_loss = np.nanmean(A_to_B_loss_sum) \n",
    "    print('Averaged loss is: '+ str(average_loss))\n",
    "    return global_step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0b32db",
   "metadata": {},
   "source": [
    "# Build the testing function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105082c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the window sliding method to denoise the whole PET image. Must used it.\n",
    "# For example, if your whole image is 96x192, and our window size is 64x64, so the function will automatically sliding down\n",
    "# the whole image with a certain overlapping ratio\n",
    "\n",
    "# The window size (img_size) is shown in the \"Build the data loader using the monai library\" section.\n",
    "# img_size: the size of sliding window\n",
    "# img_num: the number of sliding window in each process, only related to your gpu memory, it will still run through the whole volume\n",
    "# overlap: the overlapping ratio\n",
    "\n",
    "# Set up the step# for your inference\n",
    "consistency_num = 3\n",
    "steps = np.round(np.linspace(1.0, 150.0, num=consistency_num))\n",
    "def diffusion_sampling(Low_dose,A_to_B_model):\n",
    "    sampled_images = karras_sample(\n",
    "                        consistency,\n",
    "                        A_to_B_model,\n",
    "                        shape=Low_dose.shape,\n",
    "                        condition=Low_dose,\n",
    "                        sampler=\"multistep\",\n",
    "                        steps = 151,\n",
    "                        ts = steps,\n",
    "                        device = device)\n",
    "    return sampled_images\n",
    "\n",
    "# Patch-based inference parameter\n",
    "overlap = 0.75\n",
    "mode ='constant'\n",
    "back_ground_intensity = -1\n",
    "Inference_patch_number_each_time = 40\n",
    "from monai.inferers import SlidingWindowInferer\n",
    "inferer = SlidingWindowInferer(patch_size, Inference_patch_number_each_time, overlap=overlap,\n",
    "                               mode =mode ,cval = back_ground_intensity, sw_device=device,device = device)\n",
    "\n",
    "def evaluate(Consistency_network,epoch,path,data_loader1,best_loss,global_step):\n",
    "    Consistency_network.eval()\n",
    "    prediction = []\n",
    "    true = []\n",
    "    img = []\n",
    "    loss_all = []\n",
    "    aa = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(data_loader1):\n",
    "\n",
    "                Low_dose =  data['image'].to(device)        \n",
    "                High_dose = data['label'].to(device)\n",
    "                \n",
    "                High_dose_samples = inferer(Low_dose,diffusion_sampling,Consistency_network)  \n",
    "                loss = metric(High_dose_samples,High_dose)\n",
    "                print('MSE: '+ str(loss))\n",
    "                img.append(Low_dose.cpu().numpy())\n",
    "                true.append(High_dose.cpu().numpy())\n",
    "                prediction.append(High_dose_samples.cpu().numpy())    \n",
    "                loss_all.append(loss.cpu().numpy())\n",
    "                \n",
    "        print('optimization time: '+ str(time.time()-aa))\n",
    "        print('average loss: '+str(np.mean(loss_all)))\n",
    "        return np.mean(loss_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc474b1",
   "metadata": {},
   "source": [
    "# Start the training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311f54bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your data folder\n",
    "training_set1 = CustomDataset(['C:\\Research\\Diffusion model PET low to high\\data\\pet_38_aligned\\imagesTr_full_2d/'],train_flag=True)\n",
    "testing_set1 = CustomDataset(['C:\\Research\\Diffusion model PET low to high\\data\\pet_38_aligned\\imagesTs_full_2d/'],train_flag=False)\n",
    "\n",
    "# Enter your data reader parameters\n",
    "params = {'batch_size': BATCH_SIZE_TRAIN,\n",
    "          'shuffle': True,\n",
    "          'pin_memory': True,\n",
    "          'drop_last': False}\n",
    "train_loader1 = torch.utils.data.DataLoader(training_set1, **params)\n",
    "\n",
    "params = {'batch_size': 320,\n",
    "          'shuffle': False,\n",
    "          'pin_memory': True,\n",
    "          'drop_last': False}\n",
    "test_loader1 = torch.utils.data.DataLoader(testing_set1, **params)\n",
    "# Enter your total number of epoch\n",
    "N_EPOCHS = 500\n",
    "\n",
    "# Enter the address you save the checkpoint and the evaluation examples\n",
    "path =\"C:/Research/Diffusion model PET low to high/result/pet_38_CM_CNN_aligned/\"\n",
    "Consistency_network_path = path+'A_to_B_ViTRes1.pt' # Use your own path6\n",
    "best_loss = 1\n",
    "global_step = 0\n",
    "if not os.path.exists(Consistency_network_path):\n",
    "  os.makedirs(Consistency_network_path) \n",
    "train_loss_history, test_loss_history = [], []\n",
    "\n",
    "# Uncomment this when you resume the checkpoint\n",
    "# Consistency_network.load_state_dict(torch.load(Consistency_network_path),strict=True) \n",
    "Consistency_network_model_ema = copy.deepcopy(Consistency_network)\n",
    "for epoch in range(0, N_EPOCHS):\n",
    "    print('Epoch:', epoch)\n",
    "    start_time = time.time() \n",
    "    global_step = train(Consistency_network,Consistency_network_model_ema, Consistency_model_optimizer,\n",
    "                        train_loader1,consistency,global_step)\n",
    "    print('Execution time:', '{:5.2f}'.format(time.time() - start_time), 'seconds')\n",
    "    if epoch % 1 == 0:\n",
    "        average_loss = evaluate(Consistency_network,epoch,path,test_loader1,best_loss,global_step)\n",
    "        if average_loss < best_loss:\n",
    "            print('Save the latest best model')\n",
    "            torch.save(Consistency_network.state_dict(), Consistency_network_path)\n",
    "            best_loss = average_loss\n",
    "print('Execution time')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
